# Refactoring Plan

This document outlines a plan for refactoring the distributed file system codebase. The goal is to improve code organization, clarify package boundaries, and establish a clear communication protocol between system components.

## 1. Existing Features

Based on the current codebase, the following features are implemented:

*   **Client-Facing Operations (via Coordinator):**
    *   **File Upload:** A client can request to upload a file. The coordinator determines the chunking strategy and assigns data nodes for each chunk.
    *   **File Download:** A client can request to download a file. The coordinator provides the locations of the file's chunks.
    *   **Upload Confirmation:** After uploading chunks to data nodes, the client confirms the upload with the coordinator to commit the file's metadata.
    *   *(Stubbed)*: `DeleteFile` and `ListFiles` operations are defined in the API but not implemented.

*   **Coordinator:**
    *   **Node Management:** Manages a list of active data nodes. Data nodes register themselves and send periodic heartbeats. The coordinator tracks node status (`LastSeen`, `HealthStatus`) and cluster topology changes with a versioning system.
    *   **Chunk Placement:** For an upload request, it selects a set of data nodes (currently 3) for each chunk to be stored on.
    *   **Metadata Management:** It stores metadata about files and their corresponding chunks. It uses a session-based approach to track ongoing uploads before committing them.

*   **Data Node:**
    *   **Chunk Storage:** Stores raw chunk data.
    *   **gRPC Service:** Exposes a `DataNodeService` for chunk operations.
    *   **Chunk Upload Handshake:** A two-phase process (`PrepareChunkUpload` then `UploadChunkStream`) for receiving chunks.
    *   **Chunk Replication:** A data node that receives a chunk from a client is responsible for replicating it to other data nodes in parallel (`paralellReplicate`). This is triggered by the `propagate` flag in the `UploadChunkRequest`.
    *   **Heartbeating:** Periodically sends heartbeats to the coordinator.
    *   *(Partially Implemented)*: Download and Delete RPCs are defined, but their usage is not fully clear from the server implementations.

## 2. Package Boundaries

The current package structure and their responsibilities are:

*   `cmd/*`: Main entry points for the `client`, `coordinator`, and `datanode` binaries. They handle configuration parsing and service initialization.
*   `pkg/proto`: Contains the Protobuf definitions (`.proto`) which act as the definitive API contract between all services. It also contains the Go code generated by `protoc`.
*   `internal/coordinator`: Contains all business logic for the coordinator. `server.go` implements the `CoordinatorService` gRPC service.
*   `internal/datanode`: Contains all business logic for the data node. `server.go` implements the `DataNodeService` gRPC service. `replication.go` handles the logic for replicating chunks to peers.
*   `internal/client`: Contains the client-side logic for interacting with the coordinator and data nodes.
*   `internal/common`: A shared kernel of code used across different services. This includes the `NodeManager`, data stream utilities (`Streamer`), and common data types.
*   `internal/storage`: Abstractions for storage. It appears to be in early stages.
*   `api/http`: An HTTP/REST interface. Its purpose and relationship with the core gRPC-based system are unclear.

## 3. Design Discussion: A Two-Plane Architecture

The system's architecture will be formally divided into two distinct communication "planes" to ensure separation of concerns, security, and testability.

### Plane 1: Data & Core Control Plane (gRPC)

This plane is the high-performance engine of the distributed file system, handling all core operations over a **single, unified gRPC server** on each node.

*   **Technology:** gRPC.
*   **Responsibilities:**
    *   **Data Transfer:** Client chunk uploads and downloads (`UploadChunkStream`, `DownloadChunkStream`).
    *   **Replication:** Inter-data-node chunk replication.
    *   **Core State Management:** Data node registration with the coordinator (`RegisterDataNode`) and periodic heartbeats (`DataNodeHeartbeat`).
*   **Rationale:** Using a single gRPC server for these functions is optimal for performance. The efficiency of binary protocols and native streaming is critical for the frequent, high-throughput data operations at the heart of the system.

### Plane 2: Management Plane (HTTP/REST)

This plane acts as a "sidecar" management and observability agent for each node, running on a separate, dedicated HTTP port. It does not handle any primary DFS data.

*   **Technology:** HTTP/REST with JSON.
*   **Responsibilities:**
    *   **Lifecycle Management:** Exposing simple HTTP endpoints (e.g., `/status`, `/health`, `/shutdown`) to allow external tools (like a Kubelet or deployment scripts) to manage the node's process lifecycle.
    *   **Observability & Telemetry:** Providing endpoints to expose logs (`/logs/stream`) and metrics (`/metrics`), decoupling the core DFS from specific observability tooling.
    *   **Cluster Intelligence & Awareness:** Acting as the "brain" for the node's place in the cluster. This includes handling complex logic like subscribing to leader election updates for a highly-available coordinator, and ensuring the core gRPC client is always aware of the active coordinator.
*   **Rationale:** This separates the complex logic of cluster membership and observability from the core file system operations. It makes the main DFS services simpler and more focused, while providing a standard, easy-to-use interface for management and external integration.

## 4. Proposed Refactoring Steps

The refactoring will be executed in phases, prioritizing the establishment of the new architecture.

1.  **Phase 1: Establish the Cluster Package.**
    *   Create a new `internal/cluster` package.
    *   Move `internal/common/node_manager.go` and its mock into this new package.
    *   The `cluster` package will be responsible for managing the state of the cluster (node lists) and the client connections used for inter-node gRPC communication.
    *   Refactor the `coordinator` and `datanode` to delegate all node list management and peer communication to this package.

2.  **Phase 2: Introduce the Management Plane.**
    *   Create a new `internal/management` package.
    *   Implement a basic HTTP server in this package.
    *   Add placeholder endpoints for lifecycle (`/status`) and observability (`/metrics`).
    *   Update `cmd/datanode/main.go` and `cmd/coordinator/main.go` to launch this new Management server as a separate goroutine alongside the primary gRPC server.

3.  **Phase 3: Implement Advanced Features (Future Work).**
    *   Implement a robust persistence layer for the coordinator's metadata store.
    *   Add comprehensive, resilient error handling (retries, timeouts) to the control path gRPC clients in the `cluster` package.
    *   Build out advanced features in the `management` package, such as leader election tracking.

4.  **Phase 4: Consolidate & Finalize (Future Work).**
    *   Implement the stubbed API methods (`DeleteFile`, `ListFiles`).
    *   Refactor configuration into a unified system using a library like Viper. This is postponed until the core architecture is stable.

## 5. Open Questions & Analysis Needed

*   **What is the exact purpose of the `api/http` server?**
    *   **Answer:** The files in `dfs/api/http` (`handlers.go`, `server.go`, `middleware.go`) are all empty placeholders. This indicates the HTTP server is a non-functional scaffold for a feature that has not been implemented yet.
*   **What is the persistence strategy?** The `metaStore` in the coordinator and the chunk storage in the data node are used, but their implementations (`internal/storage/*`) are minimal. How are they configured and where do they store data? This needs to be clarified.
    *   **Answer:** The persistence strategy is inconsistent and incomplete:
        *   **Data Node (Chunk Storage):** Persistence is handled by `internal/storage/chunk/ChunkDiskStorage`. It stores chunk data and headers as individual files on the local filesystem in a specified `RootDir`. It uses a nested directory structure derived from the chunk ID to organize the files (e.g., `RootDir/ab/cd/abcd...`). This appears to be a functional, if simple, implementation.
        *   **Coordinator (Metadata Storage):** Persistence is handled by `internal/storage/metadata/MetadataDiskStorage`. However, this implementation is a **stub**. It does not write any data to disk; all its methods are empty. This means any file metadata held by the coordinator is stored **in-memory only** and will be lost upon restart. This is a critical gap in the system's design.
*   **Error Handling and Retries:** How are transient network errors handled? The `streamer` in `internal/common` has a `MaxChunkRetries` setting, suggesting some retry logic exists, but a more comprehensive strategy should be designed.
    *   **Answer:** Error handling is implemented robustly but only in one part of the system.
        *   **Data Path (Chunk Streaming):** The `internal/common/Streamer` component has a solid retry mechanism for sending chunk data. It retries on failed ACKs or data mismatches with exponential backoff. It also performs end-to-end checksum validation to ensure data integrity.
        *   **Control Path (RPCs):** There appears to be **no systematic retry logic** for control path communications, such as a client calling the coordinator or a data node sending a heartbeat. Failures in these operations are likely to be immediate and fatal to the operation. A comprehensive refactoring should add resiliency (retries, timeouts, circuit breakers) to these interactions as well.
*   **Configuration Details:** What are all the configurable parameters? Consolidating them (Step 4 above) will help answer this.
    *   **Answer:** The configuration is scattered across default values in code and environment variables. A full list of parameters is:
        *   **Coordinator:**
            *   `ID`: (string) Auto-generated UUID.
            *   `Host`: (string) Default: "localhost".
            *   `Port`: (int) Default: 8080.
            *   `ChunkSize`: (int) Default: 8MB.
            *   `Replication.Factor`: (int) Default: 3.
            *   `Metadata.CommitTimeout`: (time.Duration) Default: 15 minutes.
        *   **Data Node:**
            *   `Info.ID`: (string) Auto-generated UUID.
            *   `Info.Host`: (string) Env: `DATANODE_HOST`, Default: "0.0.0.0".
            *   `Info.Port`: (int) Default: 8081.
            *   `Info.Capacity`: (int64) Default: 10GB.
            *   `DiskStorage.RootDir`: (string) Env: `DISK_STORAGE_BASE_DIR`, Default: "/app/data".
            *   `Session.SessionTimeout`: (time.Duration) Default: 1 minute.
            *   `Replication.ReplicateTimeout`: (time.Duration) Default: 10 minutes.
        *   **Data Node (for connecting to Coordinator):**
            *   `COORDINATOR_HOST`: (string) Env, Default: "coordinator".
            *   `COORDINATOR_PORT`: (int) Env, Default: 8080.
        *   **Streamer (Data Transfer):**
            *   `MaxChunkRetries`: (int) Default: 3.
            *   `ChunkStreamSize`: (int) Default: 256KB.
            *   `BackpressureTime`: (time.Duration) Default: 10ms.
    *   This confirms that **Step 4 (Consolidate Configuration)** in the proposed plan is necessary to make the system easier to configure and deploy.